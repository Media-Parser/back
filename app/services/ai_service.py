# app/services/ai_service.py
import re
from openai import OpenAI
from typing import Optional, List, Tuple
from motor.motor_asyncio import AsyncIOMotorClient
from app.core.config import Settings 
from langgraph.graph import StateGraph, END
from app.services.node import (
    GraphState,
    detect_injection,
    analyze_sentiment_bias,
    plan_retrieval_node,
    standard_retrieval_node,
    balanced_retrieval_node,
    grade_and_filter_node,
    generate_response_node,
    generate_titles_node,
    generate_suggestion_node,
)

ATLAS_URI = Settings.ATLAS_URI
client = AsyncIOMotorClient(ATLAS_URI)
db = client['uploadedbyusers']
openai_client = OpenAI(api_key=Settings.OPENAI_API_KEY)

# ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
async def get_document_content(doc_id: str) -> Optional[dict]:
    temp_doc = await db["temp_docs"].find_one({"doc_id": doc_id})
    if temp_doc:
        return {
            "title": temp_doc.get("title", ""),
            "contents": temp_doc.get("contents", "")
        }
    doc = await db["docs"].find_one({"doc_id": doc_id})
    if doc:
        return {
            "title": doc.get("title", ""),
            "contents": doc.get("contents", "")
        }
    return None

# ì¢…ì†ì„± ë¬¸ì œ ì—†ìœ¼ë©´ 03_context ìª½ìœ¼ë¡œ ì´ë™ ì˜ˆì •
async def retrieve_document_node(state: GraphState) -> dict:
    print("--- ë…¸ë“œ ì‹¤í–‰: retrieve_document_node ---")
    doc_id = state.get("doc_id")
    if not doc_id:
        return {"context": ""}
    doc_to_process = await db["temp_docs"].find_one({"doc_id": doc_id}) \
        or await db["docs"].find_one({"doc_id": doc_id})
    if doc_to_process:
        formatted_context = f"[ë¬¸ì„œ ì œëª©]\n{doc_to_process.get('title', '')}\n\n[ë¬¸ì„œ ë‚´ìš©]\n{doc_to_process.get('contents', '')}"
        return {"context": formatted_context}
    return {"context": "ì˜¤ë¥˜: í•´ë‹¹ IDì˜ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

async def should_retrieve_conditionally(state: GraphState) -> str:
    strategy = state.get("plan", {}).get("strategy", "no_retrieval")
    return {
        "standard_retrieval": "standard_retriever",
        "balanced_retrieval": "balanced_retriever",
        "title_generation": "generate_titles",
        "no_retrieval": "generate"
    }.get(strategy, "__end__")
    
def guardrails_node(state: GraphState) -> GraphState:
    print("--- ë…¸ë“œ ì‹¤í–‰: guardrails ---")
    question = state["question"]
    if "ìœ„í—˜" in detect_injection(question):
        return {**state, "generation": "ì˜¤ë¥˜: ë¶€ì ì ˆí•œ ì§ˆë¬¸ì…ë‹ˆë‹¤ (í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜)."}
    bias_result = analyze_sentiment_bias(question)
    if any(k in bias_result for k in ["ê³µê²©", "ë¹„ë‚œ", "í˜ì˜¤"]):
        return {**state, "generation": f"ì˜¤ë¥˜: ë¶€ì ì ˆí•œ ì§ˆë¬¸ì…ë‹ˆë‹¤ (ê°ì§€ëœ í¸í–¥: {bias_result})."}
    return state

def check_guardrails(state: GraphState) -> str:
    return "__end__" if state.get("generation", "").startswith("ì˜¤ë¥˜") else "plan_retrieval"


# ëŒ€í™” íë¦„ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„±
# def build_messages_with_history(
#     chat_history: List[dict],  # [{question: {...}, answer: "..."}]
#     user_message: str,
#     selected_text: Optional[str] = None,
#     doc_content: Optional[dict] = None
# ):
#     system_prompt = {
#             "role": "system",
#             "content": """
#         ë‹¹ì‹ ì€ ê¸°ì‚¬ ì‘ì„±, ê¸°ì‚¬ ìš”ì•½, ê¸°ì‚¬ ì œëª© ì¶”ì²œ ë“± ë¯¸ë””ì–´ ì‘ì—…ì— íŠ¹í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•˜ëŠ” ì—´ë¦° íƒœë„ë¡œ, ë§¥ë½ì„ ì˜ ì´í•´í•´ì„œ ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì„¸ìš”.

#         - **ëª¨ë“  ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹**ìœ¼ë¡œ, ë‹¨ë½ë³„ë¡œ ë¹ˆ ì¤„ì„ ì¶©ë¶„íˆ ë„£ì–´ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.
#         - ì˜ˆì‹œ, ë¦¬ìŠ¤íŠ¸, í‘œ, ì¸ìš©êµ¬ ë“± ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.
#         - í•­ìƒ ì‚¬ì‹¤ì— ê·¼ê±°í•œ ë‹µë³€ì„ ì œê³µí•˜ê³ , ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ì¶”ì •ì´ í•„ìš”í•œ ë‚´ìš©ì€ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”.
#         - ë¯¼ê°í•˜ê±°ë‚˜ ë…¼ë€ ì†Œì§€ê°€ ìˆëŠ” ì´ìŠˆëŠ” ë°˜ë“œì‹œ ì¤‘ë¦½ì ì¸ ì‹œê°ì„ ìœ ì§€í•˜ì„¸ìš”.
#         - ê°œì¸ì •ë³´, ê³¼ë„í•œ ì¶”ì¸¡, ë„ˆë¬´ ì£¼ê´€ì  í‰ê°€ëŠ” ì‚¼ê°€ì„¸ìš”.
#         - ì§ˆë¬¸ì´ ì˜ì–´ë©´ ì˜ì–´, í•œê¸€ì´ë©´ í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”.

#         ---

#         **ê¸°ì‚¬ ì œëª© ì¶”ì²œ, ë³€ê²½, ìš”ì•½, ì œëª©ê³¼ ê´€ë ¨ëœ ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë¼ë²¨ì„ í•œ ì¤„ì— ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ì œëª©ë§Œì„ í°ë”°ì˜´í‘œ("...")ë¡œ ê°ì‹¸ í•œ ì¤„ì— ì¨ ì£¼ì„¸ìš”.**

#         - **ë°˜ë“œì‹œ ë¼ë²¨ì„ ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œ ì¨ì£¼ì„¸ìš”**  
#             - `ë³€ê²½ ì œëª© ì œì•ˆ:`
#             - `ì¶”ì²œ ì œëª©:`
#             - `ì œì•ˆí•˜ëŠ” ê¸°ì‚¬ ì œëª©:`
#             - `ê¸°ì‚¬ ì œëª© ì¶”ì²œ:`
#         - **ë°˜ë“œì‹œ ì•„ë˜ì²˜ëŸ¼ í•œ ì¤„ì—ë§Œ!**
#             - ì˜ˆì‹œ: ë³€ê²½ ì œëª© ì œì•ˆ: "ë¯¸ë˜ ì‚°ì—…ì„ ì´ë„ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í˜"
#             - ì˜ˆì‹œ: ì¶”ì²œ ì œëª©: "AI í˜ì‹ , ì‚°ì—…ì„ ë°”ê¾¸ë‹¤"
#         - **ì—¬ëŸ¬ ê°œ ì¶”ì²œ ì‹œ ë°˜ë“œì‹œ 1. ... 2. ...** í˜•ì‹ìœ¼ë¡œ í•œ ì¤„ì”© ì¨ì£¼ì„¸ìš”.

#         ---

#         **ë¬¸ì¥/ë¬¸ë‹¨/ë³¸ë¬¸/ë‚´ìš© ì¶”ì²œë„ ë°˜ë“œì‹œ ë¼ë²¨ë¡œ í‘œì‹œí•´ ì£¼ì„¸ìš”.**
#         - ì ìš©í•  ë¬¸ì¥: "..."
#         - ì¶”ì²œ ë¬¸ì¥: "..."
#         - ë³€ê²½ ë¬¸ì¥ ì œì•ˆ: "..."

#         ---

#         **"ìˆ˜ì •", "ì¶”ì²œ", "ë³€ê²½", "ë‹¤ë“¬ê¸°" ìš”ì²­ì´ í¬í•¨ëœ ì§ˆë¬¸ì— ë‹µí•  ë• ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”.**

#         ğŸ”„ **ìˆ˜ì • ì œì•ˆ**

#         **Before:**  
#         (ìˆ˜ì • ì „ ë¬¸ì¥)

#         **After:**  
#         (ìˆ˜ì • í›„ ë¬¸ì¥ â€” ë°”ë€ ë¶€ë¶„ì„ **êµµê²Œ** í˜¹ì€ ==ë°‘ì¤„==ë¡œ ê°•ì¡°)

#         > ë³€ê²½ ì´ìœ : (ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…. ë°˜ë“œì‹œ Afterì™€ í•œ ë¸”ë¡ì— í¬í•¨)

#         ---

#         **í¬ë§·ì„ ì–´ê¸°ë©´ ì‚¬ìš©ìê°€ ì ìš©/ë³µì‚¬ë¥¼ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ í¬ë§·ì„ ì§€í‚¤ì„¸ìš”!**

#         **ë‹¨, ê¸°íƒ€ ì„¤ëª…/ë¶„ì„/ë¹„í‰/ìš”ì•½ ë“±ì€ ê¸°ì¡´ ë§ˆí¬ë‹¤ìš´ ê·œì¹™ì„ ì§€í‚¤ì„¸ìš”.**

#         """
#     }
#     messages = [system_prompt]
#     for qa in chat_history:
#         q = qa.get("question", {})
#         q_txt = q.get("message") if isinstance(q, dict) else str(q)
#         # selection ë³´ì—¬ì£¼ê³  ì‹¶ìœ¼ë©´ ì¶”ê°€
#         if q.get("selected_text"):
#             q_txt += f"\n\n{q['selected_text']}"
#         messages.append({"role": "user", "content": q_txt})
#         messages.append({"role": "assistant", "content": qa.get("answer", "")})
#     # ë§ˆì§€ë§‰: í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
#     new_q = user_message
#     if selected_text:
#         new_q += f"\n\n{selected_text}"
#     if doc_content and isinstance(doc_content, dict):
#         title = doc_content.get("title", "")
#         contents = doc_content.get("contents", "")
#         new_q += (
#             f"\n\n[ë¬¸ì„œ ì œëª©]\n{title}\n\n"
#             f"[ë¬¸ì„œ ë‚´ìš©]\n{contents}\n"
#         )
#     elif doc_content:
#         new_q += f"\n\n(ì „ì²´ ê¸°ì‚¬ ë‚´ìš©: {doc_content})"
#     messages.append({"role": "user", "content": new_q})
#     return messages

# # í›„ì† ì§ˆë¬¸ ìƒì„±
# async def generate_suggestion(answer: str, user_message: str) -> str:
#     """
#     AI ë‹µë³€ê³¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬ í›„ì†ìœ¼ë¡œ í•  ë§Œí•œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì¤Œ
#     """
#     prompt = (
#         f"ì•„ë˜ëŠ” ê¸°ì‚¬ ê´€ë ¨ AIê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\n\n"
#         f"ì§ˆë¬¸: {user_message}\n"
#         f"ë‹µë³€: {answer}\n\n"
#         "ë§Œì•½ ì‚¬ìš©ìê°€ ì´ì–´ì„œ ê¶ê¸ˆí•´í•  ë§Œí•œ ë‹¤ìŒ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”. "
#         "ì‹¤ì œ ì‚¬ìš©ìì˜ ì…ì¥ì—ì„œ, ë‹µë³€ì„ ë³´ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§ˆë§Œí•œ ì¶”ê°€ ì§ˆë¬¸ì„ ì˜ˆì‹œë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. "
#         "ì§ˆë¬¸ë¬¸ ëì—ëŠ” ë°˜ë“œì‹œ '?'ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”."
#     )
#     completion = openai_client.chat.completions.create(
#         model="gpt-3.5-turbo",
#         messages=[
#             {"role": "system", "content": "ë„ˆëŠ” ê¸°ì‚¬ ê´€ë ¨ AI ëŒ€í™”ì˜ íë¦„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì£¼ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì•¼."},
#             {"role": "user", "content": prompt}
#         ],
#         temperature=0.5,
#         max_tokens=500
#     )
#     print("=== AI SUGGESTION ===\n",completion.choices[0].message.content)
#     suggestion = completion.choices[0].message.content.strip()
#     suggestion = re.sub(r"^(í›„ì† ì§ˆë¬¸:|Q:|ì§ˆë¬¸:)\s*", "", suggestion)
#     return suggestion

# AI ì‘ë‹µ ìƒì„±
async def generate_ai_response(
    message: str,
    doc_id: str,
    selected_text: Optional[str] = None,
    use_full_document: bool = False
) -> Tuple[str, Optional[str]]:
    """
    LangGraph íŒŒì´í”„ë¼ì¸ì„ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ê³¼ ì¶”ì²œì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        graph_inputs = {
            "question": message,
            "original_question": message,
            "doc_id": doc_id,
            "selected_text": selected_text,
            "documents": [],
            "generation": "",
            "context": "",
            "retries": 0,
            "plan": None,
            "suggestion": None,
            "value_type": None,
            "apply_value": None,
        }
        final_state = await graph_app.ainvoke(graph_inputs)
        answer = final_state.get("generation", "")
        suggestion = final_state.get("suggestion", "")
        return answer, suggestion
    except Exception as e:
        return f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}", None
    
    # try:
    #     # 1. ìµœê·¼ [limit]ê°œ ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
    #     from app.services.chat_service import get_chat_history_for_prompt
    #     chat_history = await get_chat_history_for_prompt(doc_id, limit=3)

    #     # 2. ì „ì²´ ë¬¸ì„œ ë‚´ìš© í•„ìš” ì‹œ
    #     doc_content = None
    #     if use_full_document or not selected_text:
    #         doc_content = await get_document_content(doc_id)

    #     # 3. messages êµ¬ì„±
    #     messages = build_messages_with_history(
    #         chat_history=chat_history,
    #         user_message=message,
    #         selected_text=selected_text,
    #         doc_content=doc_content
    #     )

    #     # 4. OpenAI í˜¸ì¶œ
    #     completion = openai_client.chat.completions.create(
    #         model="gpt-3.5-turbo",
    #         messages=messages,
    #         temperature=0.7,
    #         max_tokens=2000
    #     )
    #     answer = (completion.choices[0].message.content or "").strip()

    #     # 5. ì¶”ì²œì§ˆë¬¸ ìƒì„±
    #     suggestion = await generate_suggestion(answer, message)
    #     return answer, suggestion

    # except Exception as e:
    #     error_message = f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    #     return error_message, None
    
    
    # --- LangGraph ì •ì˜ ---
graph_builder = StateGraph(GraphState)
graph_builder.add_node("retrieve_document", retrieve_document_node)
graph_builder.add_node("guardrails", guardrails_node)
graph_builder.add_node("plan_retrieval", plan_retrieval_node)
graph_builder.add_node("standard_retriever", standard_retrieval_node)
graph_builder.add_node("balanced_retriever", balanced_retrieval_node)
graph_builder.add_node("grade_and_filter", grade_and_filter_node)
graph_builder.add_node("generate", generate_response_node)
graph_builder.add_node("generate_titles", generate_titles_node)
graph_builder.add_node("generate_suggestion_node", generate_suggestion_node)

# --- ì—£ì§€ ì—°ê²° ---
graph_builder.set_entry_point("retrieve_document")
graph_builder.add_edge("retrieve_document", "guardrails")
graph_builder.add_conditional_edges("guardrails", check_guardrails, {"plan_retrieval": "plan_retrieval", "__end__": END})
graph_builder.add_conditional_edges("plan_retrieval", should_retrieve_conditionally, {
    "standard_retriever": "standard_retriever",
    "balanced_retriever": "balanced_retriever",
    "generate_titles": "generate_titles",
    "generate": "generate",
    "__end__": END
})
graph_builder.add_edge("standard_retriever", "grade_and_filter")
graph_builder.add_edge("balanced_retriever", "grade_and_filter")

# rewrite ë¡œì§ ì¼ë‹¨ í•„ìš” ì—†ìŒ
graph_builder.add_edge("grade_and_filter", "generate") 
# graph_builder.add_conditional_edges("grade_and_filter", decide_to_generate_or_rewrite, {
#     "generate": "generate",
#     "rewrite_query": "rewrite_query"
# })
# graph_builder.add_edge("rewrite_query", "generate")

graph_builder.add_edge("generate", END)
graph_builder.add_edge("generate_titles", END)

# ì»´íŒŒì¼
graph_app = graph_builder.compile()


if __name__ == "__main__":
    import asyncio

    async def main():
        print("\n--- LangGraph RAG ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ---")
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit'ì„ ì…ë ¥í•˜ì„¸ìš”.")

        while True:
            question = input("\nì§ˆë¬¸: ")
            if question.lower() in ["exit", "quit"]:
                print("ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            inputs = {"question": question, "documents": [], "value_type": "content"}
            final_state = await graph_app.ainvoke(inputs)
            
            generation = final_state.get("generation")
            if not generation:
                print("\n[AI ë‹µë³€]\nì£„ì†¡í•˜ì§€ë§Œ, ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.")
            else:
                print("\n[AI ë‹µë³€]")
                print(generation)
            print(final_state)
    
    asyncio.run(main())