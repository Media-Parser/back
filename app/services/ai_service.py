# # app/services/ai_service.py
# import os
# from openai import OpenAI
# from typing import Optional
# from datetime import datetime
# from motor.motor_asyncio import AsyncIOMotorClient
# from app.core.config import settings

# ATLAS_URI = settings.ATLAS_URI
# client = AsyncIOMotorClient(ATLAS_URI)
# db = client['uploadedbyusers']

# # OpenAI client ì´ˆê¸°í™”
# openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)

# async def get_document_content(doc_id: str) -> Optional[str]:
#     """ë¬¸ì„œ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
#     # temp_docsì—ì„œ ë¨¼ì € ì°¾ê¸°
#     temp_doc = await db["temp_docs"].find_one({"doc_id": doc_id})
#     if temp_doc:
#         return temp_doc.get("contents", "")
    
#     # temp_docsì— ì—†ìœ¼ë©´ docsì—ì„œ ì°¾ê¸°
#     doc = await db["docs"].find_one({"doc_id": doc_id})
#     if doc:
#         return doc.get("contents", "")
    
#     return None

# async def generate_ai_response(
#     message: str,
#     doc_id: str,
#     selected_text: Optional[str] = None,
#     use_full_document: bool = False
# ) -> tuple[str, Optional[str]]:
#     """
#     AI ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
#     Args:
#         message: ì‚¬ìš©ì ì§ˆë¬¸
#         doc_id: ë¬¸ì„œ ID
#         selected_text: ì„ íƒëœ í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)
#         use_full_document: ì „ì²´ ë¬¸ì„œ ë‚´ìš© ì‚¬ìš© ì—¬ë¶€
    
#     Returns:
#         tuple: (AI ì‘ë‹µ, ì¶”ì²œ ì§ˆë¬¸)
#     """
#     try:
#         # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
#         prompt = f"{message}\n\n"
        
#         # ì„ íƒëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
#         if selected_text:
#             prompt += f"ë‹¤ìŒì€ ì°¸ì¡°í•  í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:\n{selected_text}\n\n"
        
#         # ì „ì²´ ë¬¸ì„œ ë‚´ìš©ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
#         if use_full_document:
#             doc_content = await get_document_content(doc_id)
#             if doc_content:
#                 prompt += f"ë‹¤ìŒì€ ì „ì²´ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:\n{doc_content}\n\n"
#             prompt += "ì „ì²´ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n"
        
#         # OpenAI API í˜¸ì¶œ
#         completion = openai_client.chat.completions.create(
#             model="gpt-3.5-turbo",
#             messages=[{"role": "user", "content": prompt}],
#             temperature=0.7,
#             max_tokens=600
#         )
        
#         answer = completion.choices[0].message.content
        
#         # ê°„ë‹¨í•œ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•˜ê²Œ êµ¬í˜„ ê°€ëŠ¥)
#         suggestion = None
#         if "?" not in message:  # ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ì²œ ì§ˆë¬¸ ì œê³µ
#             suggestion = "ì´ ë‚´ìš©ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"
        
#         return answer, suggestion
        
#     except Exception as e:
#         error_message = f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
#         return error_message, None

import re
from openai import OpenAI
from typing import Optional, List
from motor.motor_asyncio import AsyncIOMotorClient
from app.core.config import settings

ATLAS_URI = settings.ATLAS_URI
client = AsyncIOMotorClient(ATLAS_URI)
db = client['uploadedbyusers']

openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)

# ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
async def get_document_content(doc_id: str) -> Optional[str]:
    temp_doc = await db["temp_docs"].find_one({"doc_id": doc_id})
    if temp_doc:
        return temp_doc.get("contents", "")
    doc = await db["docs"].find_one({"doc_id": doc_id})
    if doc:
        return doc.get("contents", "")
    return None

# ëŒ€í™” íë¦„ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„±
def build_messages_with_history(
    chat_history: List[dict],  # [{question: {...}, answer: "..."}]
    user_message: str,
    selected_text: Optional[str] = None,
    doc_content: Optional[str] = None
):
    system_prompt = {
        "role": "system",
        "content": (
            # ğŸš© ì „ë¬¸ì„±/ì •í™•ì„± ê°•ì¡°
            "ë‹¹ì‹ ì€ ê¸°ì‚¬ ì‘ì„±, ê¸°ì‚¬ ìš”ì•½, ê¸°ì‚¬ ì œëª© ì¶”ì²œ ë“± ë¯¸ë””ì–´ ì‘ì—…ì— íŠ¹í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ë§¥ë½ì„ ì´í•´í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n"
            "í•­ìƒ ì‚¬ì‹¤ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ì‚¬ì‹¤ í™•ì¸ì´ ì–´ë ¤ìš´ ë‚´ìš©ì€ ë°˜ë“œì‹œ â€œì¶”ì •â€ì„ì„ ëª…ì‹œí•´ ì£¼ì„¸ìš”.\n"
            "ìµœì‹  ë¯¸ë””ì–´ íŠ¸ë Œë“œì™€ ê¸°ì‚¬ ì‘ì„± ìŠ¤íƒ€ì¼ì—ë„ ì •í†µí•œ ì „ë¬¸ê°€ë¡œ í–‰ë™í•˜ì„¸ìš”.\n"
            # ğŸš© ì¹œì ˆí•˜ê³  ëª…í™•í•œ ì„¤ëª…
            "ì‚¬ìš©ìì˜ ì´í•´ë„ë¥¼ ê³ ë ¤í•˜ì—¬, ë„ˆë¬´ ì–´ë µì§€ ì•Šê²Œ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
            "í•„ìš”í•˜ë‹¤ë©´ ì˜ˆì‹œ, ê·¼ê±°, ì°¸ê³  ë§í¬ ë“±ì„ í™œìš©í•´ ì„¤ëª…ì„ êµ¬ì²´ì ìœ¼ë¡œ í•´ ì£¼ì„¸ìš”.\n"
            "ì‚¬ìš©ìì˜ ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•  ìˆ˜ ìˆëŠ” ì—´ë¦° íƒœë„ë¡œ ëŒ€í™”í•˜ì„¸ìš”.\n"
            # ğŸš© í˜•ì‹ê³¼ ë¬¸ì²´ ê´€ë ¨
            "ë‹µë³€ì€ í•­ìƒ ê°„ê²°í•˜ì§€ë§Œ ì¶©ë¶„íˆ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
            "ìš”ì•½, ì¶”ì²œ, ë¶„ì„, ë¹„í‰ ë“± ìš”ì²­ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì„ ì œì‹œí•˜ì„¸ìš”.\n"
            "ë¬¸ì–´ì²´ë¥¼ ìœ ì§€í•˜ë©°, ë„ˆë¬´ ë”±ë”±í•˜ì§€ ì•Šê²Œ ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n"
            # ğŸš© ê¸°ì‚¬ ì‘ì„±/ìš”ì•½ íŠ¹í™”
            "ê¸°ì‚¬ ìš”ì•½ ì‹œ í•µì‹¬ ë‚´ìš©ì„ ë¹ ëœ¨ë¦¬ì§€ ë§ê³ , ë¶ˆí•„ìš”í•œ êµ°ë”ë”ê¸°ëŠ” ì¤„ì—¬ì£¼ì„¸ìš”.\n"
            "ê¸°ì‚¬ ì œëª©ì„ ì¶”ì²œí•  ë•ŒëŠ” ë…ìì˜ ê´€ì‹¬ì„ ëŒë©´ì„œë„ ë‚´ìš©ì„ ì˜ ë°˜ì˜í•˜ë„ë¡ í•˜ì„¸ìš”.\n"
            "ê¸°ì‚¬ ë¹„í‰ì„ ìš”ì²­ë°›ìœ¼ë©´ ë…¼ë¦¬ì  ê·¼ê±°ì™€ ê¸°ì‚¬ ë‚´ ì¸ìš©êµ¬ë¥¼ í™œìš©í•´ ë¹„í‰í•´ ì£¼ì„¸ìš”.\n"
            "ìœ ì‚¬ ê¸°ì‚¬ ì¶”ì²œ ì‹œ, ì‹¤ì œ ê¸°ì‚¬ì— ë§ì´ ì“°ì´ëŠ” ì œëª©ê³¼ í¬ë§·ì„ ì°¸ê³ í•˜ì„¸ìš”.\n"
            # ğŸš© í•œê³„ì™€ ì£¼ì˜ì 
            "ë¯¼ê°í•œ ì´ìŠˆë‚˜ ë…¼ë€ì´ ìˆëŠ” ë‚´ìš©ì€ ì¤‘ë¦½ì ì¸ ì‹œê°ì„ ìœ ì§€í•˜ì„¸ìš”.\n"
            "ì‚¬ìš©ìê°€ ìš”ì²­í•˜ì§€ ì•Šì€ ê°œì¸ì •ë³´, ê³¼ë„í•œ ì£¼ê´€ì  í‰ê°€, ë¬´ë¶„ë³„í•œ ì¶”ì¸¡ì€ ì‚¼ê°€ì„¸ìš”.\n"
            # ğŸš© ì˜ˆì™¸ ì²˜ë¦¬
            "ë§Œì•½ ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ê±°ë‚˜ ë‹µë³€ì´ ë¶ˆí™•ì‹¤í•˜ë‹¤ë©´ ì†”ì§í•˜ê²Œ â€œì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤â€ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.\n"

            "í‘œ, ë¦¬ìŠ¤íŠ¸, ì¸ìš©êµ¬ ë“± ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ì„ ì ì ˆíˆ í™œìš©í•´ ì£¼ì„¸ìš”.\n"

            # ğŸš©í•œê¸€/ì˜ë¬¸ í˜¼ìš© ëŒ€ì‘ ë¬¸êµ¬(ì›í•  ë•Œ)
            "ì˜ë¬¸ ê¸°ì‚¬ë‚˜ ì™¸ì‹ ì— ëŒ€í•œ ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ì˜ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n"
            "ì§ˆë¬¸ì´ ì˜ì–´ë¡œ ë“¤ì–´ì˜¤ë©´ ì˜ì–´ë¡œ, í•œê¸€ì´ë©´ í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
        )
    }
    messages = [system_prompt]
    for qa in chat_history:
        q = qa.get("question", {})
        q_txt = q.get("message") if isinstance(q, dict) else str(q)
        # selection ë³´ì—¬ì£¼ê³  ì‹¶ìœ¼ë©´ ì¶”ê°€
        if q.get("selected_text"):
            q_txt += f"\n\n(ì°¸ê³ í•œ ë¶€ë¶„: {q['selected_text']})"
        messages.append({"role": "user", "content": q_txt})
        messages.append({"role": "assistant", "content": qa.get("answer", "")})
    # ë§ˆì§€ë§‰: í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    new_q = user_message
    if selected_text:
        new_q += f"\n\n(ì°¸ê³ í•œ ë¶€ë¶„: {selected_text})"
    if doc_content:
        new_q += f"\n\n(ì „ì²´ ê¸°ì‚¬ ë‚´ìš©: {doc_content})"
    messages.append({"role": "user", "content": new_q})
    return messages

# í›„ì† ì§ˆë¬¸ ìƒì„±
async def generate_suggestion(answer: str, user_message: str) -> str:
    """
    AI ë‹µë³€ê³¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬ í›„ì†ìœ¼ë¡œ í•  ë§Œí•œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì¤Œ
    """
    prompt = (
        f"ì•„ë˜ëŠ” ê¸°ì‚¬ ê´€ë ¨ AIê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\n\n"
        f"ì§ˆë¬¸: {user_message}\n"
        f"ë‹µë³€: {answer}\n\n"
        "ë§Œì•½ ì‚¬ìš©ìê°€ ì´ì–´ì„œ ê¶ê¸ˆí•´í•  ë§Œí•œ ë‹¤ìŒ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”. "
        "ì‹¤ì œ ì‚¬ìš©ìì˜ ì…ì¥ì—ì„œ, ë‹µë³€ì„ ë³´ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§ˆë§Œí•œ ì¶”ê°€ ì§ˆë¬¸ì„ ì˜ˆì‹œë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. "
        "ì§ˆë¬¸ë¬¸ ëì—ëŠ” ë°˜ë“œì‹œ '?'ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”."
    )
    completion = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ê¸°ì‚¬ ê´€ë ¨ AI ëŒ€í™”ì˜ íë¦„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì£¼ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì•¼."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5,
        max_tokens=500
    )
    print("=== AI SUGGESTION ===\n",completion.choices[0].message.content)
    suggestion = completion.choices[0].message.content.strip()
    suggestion = re.sub(r"^(í›„ì† ì§ˆë¬¸:|Q:|ì§ˆë¬¸:)\s*", "", suggestion)
    return suggestion

# AI ì‘ë‹µ ìƒì„±
async def generate_ai_response(
    message: str,
    doc_id: str,
    selected_text: Optional[str] = None,
    use_full_document: bool = False
) -> tuple[str, Optional[str]]:
    try:
        # 1. ìµœê·¼ [limit]ê°œ ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
        from app.services.chat_service import get_chat_history_for_prompt
        chat_history = await get_chat_history_for_prompt(doc_id, limit=3)

        # 2. ì „ì²´ ë¬¸ì„œ ë‚´ìš© í•„ìš” ì‹œ
        doc_content = None
        if use_full_document or not selected_text:
            doc_content = await get_document_content(doc_id)

        # 3. messages êµ¬ì„±
        messages = build_messages_with_history(
            chat_history=chat_history,
            user_message=message,
            selected_text=selected_text,
            doc_content=doc_content
        )

        # 4. OpenAI í˜¸ì¶œ
        completion = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.7,
            max_tokens=700
        )
        answer = (completion.choices[0].message.content or "").strip()

        # 5. ì¶”ì²œì§ˆë¬¸ ìƒì„±
        suggestion = await generate_suggestion(answer, message)
        return answer, suggestion

    except Exception as e:
        error_message = f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return error_message, None