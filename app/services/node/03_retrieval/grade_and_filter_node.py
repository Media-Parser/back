# íŒŒì¼: 3_grade_and_filter_node.py

import torch
import torch.nn.functional as F
import numpy as np
from typing import List
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from langchain_core.documents import Document
from graph_state import GraphState
from langchain.text_splitter import RecursiveCharacterTextSplitter

load_dotenv()

# --- ì •ê·œí™” í•¨ìˆ˜ ---
def exp_normalize(x: np.ndarray) -> np.ndarray:
    b = x.max()
    y = np.exp(x - b)
    return y / y.sum()

# --- GPU ì„¤ì • ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ“¡ í˜„ì¬ ë””ë°”ì´ìŠ¤: {device}")

# --- KoBGE Cross-Encoder ë¡œë“œ ---
# ì‹¤ì œ ì„œë²„ì— ì˜¬ë¦´ ë• ë¯¸ë¦¬ ìºì‹œí•´ë‘ê³  ì“¸ ìˆ˜ ìˆê²Œ í•  ì˜ˆì •. (get_instance)
MODEL_PATH = "Dongjin-kr/ko-reranker"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)
model.eval()

# --- ë¦¬ë­í¬ í•¨ìˆ˜ ---
def rerank_documents(query: str, documents: List[Document], top_k: int = 5, batch_size: int = 8) -> List[Document]:
    print(f"ğŸ“¥ ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¦¬ë­í‚¹ ì¤‘...")

    pairs = [[query, doc.page_content] for doc in documents]
    scores = []

    for i in range(0, len(pairs), batch_size):
        batch_pairs = pairs[i:i+batch_size]
        batch_docs = documents[i:i+batch_size]

        inputs = tokenizer(batch_pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            logits = model(**inputs).logits.view(-1).float().cpu()

        batch_scores = exp_normalize(logits.numpy())
        scores.extend(zip(batch_scores, batch_docs))

    scored_docs = sorted(scores, key=lambda x: x[0], reverse=True)
    top_docs = [doc for score, doc in scored_docs[:top_k]]

    for i, (score, doc) in enumerate(scored_docs[:top_k]):
        print(f"âœ… {i+1}ìœ„ ë¬¸ì„œ (score={score:.4f}): {doc.page_content[:60]}...")

    return top_docs

# --- LangGraph ë…¸ë“œ ---
def chunk_documents(documents: List[Document], chunk_size=300, chunk_overlap=50) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_documents(documents)

# ë…¸ë“œ í•¨ìˆ˜ ê°œì„ 
def grade_and_filter_node(state: GraphState) -> GraphState:
    print("--- ë…¸ë“œ ì‹¤í–‰: 3. grade_and_filter (GPU ë¦¬ë­ì»¤ + ì²­í‚¹) ---")
    question = state.get("question")
    documents = state.get("documents")

    # 0. í˜¹ì‹œ Tuple(Document, score) êµ¬ì¡°ì´ë©´ Documentë§Œ ì¶”ì¶œ
    if documents and isinstance(documents[0], tuple):
        print("ğŸ“Œ íŠœí”Œ í˜•ì‹ ë¬¸ì„œ ê°ì§€ â†’ Documentë§Œ ì¶”ì¶œ ì¤‘")
        documents = [doc for doc, score in documents]

    if not documents:
        print("âŒ í‰ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {**state, "documents": [], "generation": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    # 1. ë¬¸ì„œ ì²­í‚¹
    chunked_docs = chunk_documents(documents)
    print(f"ğŸ“¦ ì´ {len(chunked_docs)}ê°œ ì²­í¬ë¡œ ë³€í™˜ ì™„ë£Œ.")

    # 2. ë¦¬ë­í¬
    top_k = min(5, len(chunked_docs))
    top_chunks = rerank_documents(question, chunked_docs, top_k=top_k)

    print(f"ğŸ¯ ìµœì¢… ì„ íƒ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(top_chunks)}")

    # 3. generate ìƒëµ ì‹œ â†’ í•˜ì´í¼ë§í¬ ëª©ë¡ ìƒì„±
    plan = state.get("plan", {})
    if not plan.get("generation_required"):
        # ë¬¸ì„œ ì œëª©ê³¼ url ì¶”ì¶œ
        link_lines = []
        for doc in top_chunks:
            meta = doc.metadata
            title = meta.get("title", "ì œëª© ì—†ìŒ")
            url = meta.get("url", "#")
            line = f"- [{title}]({url})"
            link_lines.append(line)
        summary_text = "\n".join(link_lines) if link_lines else "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return {**state, "documents": top_chunks, "generation": summary_text}

    # 4. í‰ì†Œì²˜ëŸ¼ ë‹¤ìŒ ë…¸ë“œ(generate)ë¡œ ë„˜ê¸¸ ê²½ìš°
    return {**state, "documents": top_chunks}
