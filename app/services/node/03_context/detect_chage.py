# service/node/03_context/detect_chage.py

import hashlib
from typing import List, Dict, Any
from textwrap import dedent
import difflib

class DocumentManager:
    """
    ë³µí•© ì²­í‚¹ ì „ëµì„ ì‚¬ìš©í•˜ê³ , ì²­í¬ ë‹¨ìœ„ì˜ ì¶”ê°€/ì‚­ì œ/ë³€ê²½ì„ ê°ì§€í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self):
        self.chunks: List[str] = []
        self.hashes: List[str] = []
        print("âœ… DocumentManagerê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def _get_hash(self, content: str) -> str:
        return hashlib.sha256(content.encode('utf-8')).hexdigest()

    def _chunk_text(self, text: str, max_chunk_size: int) -> List[str]:
        """ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , ê¸´ ì¤„ì€ ê¸€ì ìˆ˜ì— ë§ì¶° ë‹¤ì‹œ ë‚˜ëˆ„ëŠ” ë©”ì„œë“œ."""
        final_chunks = []
        lines = text.strip().splitlines()
        for line in lines:
            if len(line) <= max_chunk_size:
                final_chunks.append(line)
            else:
                for i in range(0, len(line), max_chunk_size):
                    final_chunks.append(line[i:i + max_chunk_size])
        return final_chunks

    def load_document(self, full_document: str, max_chunk_size: int = 80):
        clean_document = dedent(full_document)
        self.chunks = self._chunk_text(clean_document, max_chunk_size)
        self.hashes = [self._get_hash(chunk) for chunk in self.chunks]
        print(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ. ì´ {len(self.chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ğŸ¯ ê¹ƒí—ˆë¸Œ ìŠ¤íƒ€ì¼ diff ìƒì„± ë¡œì§ ì œê±°í•˜ê³  ë‹¨ìˆœí™”
    def find_and_update_changes(self, new_full_document: str, max_chunk_size: int = 80) -> Dict[str, List[int]]:
        """
        ì²­í¬ ë‹¨ìœ„ë¡œ ì¶”ê°€, ì‚­ì œ, ë³€ê²½ì„ ê°ì§€í•˜ê³  í•´ë‹¹ ì¸ë±ìŠ¤ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        print("\n--- [ë‹¨ìˆœ ì²­í¬ ë‹¨ìœ„] ë³€ê²½ì‚¬í•­ ê°ì§€ ì‹œì‘ ---")
        clean_document = dedent(new_full_document)
        new_chunks = self._chunk_text(clean_document, max_chunk_size)
        new_hashes = [self._get_hash(chunk) for chunk in new_chunks]

        matcher = difflib.SequenceMatcher(None, self.hashes, new_hashes)
        changes: Dict[str, List[int]] = {"modified": [], "deleted": [], "inserted": []}
        has_changes = False

        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'replace':
                # ë³€ê²½ëœ ì²­í¬ì˜ ì¸ë±ìŠ¤ë§Œ ê¸°ë¡
                changes["modified"].extend(range(j1, j2))
                has_changes = True
            elif tag == 'delete':
                changes["deleted"].extend(range(i1, i2))
                has_changes = True
            elif tag == 'insert':
                changes["inserted"].extend(range(j1, j2))
                has_changes = True
        
        if not has_changes:
            print("â„¹ï¸ ë¬¸ì„œì— ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        self.chunks = new_chunks
        self.hashes = new_hashes
        print("ğŸ”„ ë¬¸ì„œ ìƒíƒœê°€ ìµœì‹ ìœ¼ë¡œ ë™ê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return changes

    # get_context ë©”ì„œë“œëŠ” ë³€ê²½ ì—†ìŒ
    def get_context(self, focus_index: int, max_tokens: int) -> str:
        if not (0 <= focus_index < len(self.chunks)): return "ì—ëŸ¬"
        def _estimate_tokens(text: str) -> int: return len(text.split())
        context_chunks = [self.chunks[focus_index]]
        current_tokens = _estimate_tokens(self.chunks[focus_index])
        left, right = focus_index - 1, focus_index + 1
        while current_tokens < max_tokens:
            added_something = False
            if right < len(self.chunks):
                chunk_to_add = self.chunks[right]
                chunk_tokens = _estimate_tokens(chunk_to_add)
                if current_tokens + chunk_tokens <= max_tokens:
                    context_chunks.append(chunk_to_add); current_tokens += chunk_tokens; added_something = True
                right += 1
            if left >= 0:
                chunk_to_add = self.chunks[left]
                chunk_tokens = _estimate_tokens(chunk_to_add)
                if current_tokens + chunk_tokens <= max_tokens:
                    context_chunks.insert(0, chunk_to_add); current_tokens += chunk_tokens; added_something = True
                left -= 1
            if not added_something: break
        print(f"ğŸ“¦ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ (í¬ì»¤ìŠ¤: {focus_index}ë²ˆ ì²­í¬)")
        return "\n".join(context_chunks)

# --- ì‚¬ìš© ì˜ˆì‹œ ---
if __name__ == "__main__":
    manager = DocumentManager()

    # 1. ì´ˆê¸° ë¬¸ì„œ ë¡œë“œ
    initial_document = """
        ì²« ë²ˆì§¸ ì¤„ì€ ì§§ìŠµë‹ˆë‹¤.
        ë‘ ë²ˆì§¸ ì¤„ì€ ì•„ì£¼ ê¹ë‹ˆë‹¤. ì´ ì¤„ì€ ì„¤ì •ëœ ìµœëŒ€ ì²­í¬ í¬ê¸°ì¸ 80ìë¥¼ ì´ˆê³¼í•˜ë¯€ë¡œ, ë¡œë“œë  ë•Œ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ì²­í¬ë¡œ ìë™ìœ¼ë¡œ ë‚˜ë‰˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì´ í•µì‹¬ í…ŒìŠ¤íŠ¸ í¬ì¸íŠ¸ì…ë‹ˆë‹¤.
        ì„¸ ë²ˆì§¸ ì¤„ë„ ì§§ìŠµë‹ˆë‹¤."""
    
    print("\n--- 1. ì´ˆê¸° ë¬¸ì„œ ë¡œë“œ ---")
    manager.load_document(initial_document, max_chunk_size=80)

    # 2. ê¸´ ì¤„ì˜ ì¤‘ê°„ ë¶€ë¶„ë§Œ ìˆ˜ì •í•œ ë¬¸ì„œë¥¼ ì¤€ë¹„
    modified_document = """ì²« ë²ˆì§¸ ì¤„ì€ ì§§ìŠµë‹ˆë‹¤.
        ë‘ ë²ˆì§¸ ì¤„ì€ ì•„ì£¼ ê¹ë‹ˆë‹¤. ì´ ì¤„ì€ ì„¤ì •ëœ ìµœëŒ€ ì²­í¬ í¬ê¸°ì¸ 80ìë¥¼ ì´ˆê³¼í•˜ë¯€ë¡œ, ë¡œë“œë  ë•Œ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ì²­í¬ë¡œ ë°”ë€Œì–´ì„œ í…ŒìŠ¤íŠ¸ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì´ í•µì‹¬ í…ŒìŠ¤íŠ¸ í¬ì¸íŠ¸ì…ë‹ˆë‹¤.
        ì„¸"""

    # 3. ë³€ê²½ì‚¬í•­ì„ ì°¾ê³  ì¸ë±ìŠ¤ ëª©ë¡ì„ í™•ì¸
    changes_dict = manager.find_and_update_changes(modified_document, max_chunk_size=80)

    if changes_dict:
        print(f"\n--- ê°ì§€ëœ ë³€ê²½ ë‚´ì—­ (ì¸ë±ìŠ¤) ---")
        print(changes_dict)