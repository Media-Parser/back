# âœ… app/services/gen_model_service.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from app.services.prompt import build_prompt
from app.utils.parse import parse_generated_output
import sys
import torch
import os

# --- ê²½ë¡œ ì„¤ì • ---
current_file_dir = os.path.dirname(os.path.abspath(__file__))
services_dir = current_file_dir
app_dir = os.path.dirname(services_dir)
ssami_back_dir = os.path.dirname(app_dir)
ssami_dir = os.path.dirname(ssami_back_dir)
HOME_DIR = os.path.dirname(ssami_dir)

if HOME_DIR not in sys.path:
    sys.path.append(HOME_DIR)

# --- ëª¨ë¸ ê²½ë¡œ ---
MODEL_DIR = os.path.join(HOME_DIR, "ai", "finetuned_seq2seq_final_v3")
BASE_MODEL_NAME = "google/mt5-small"

# --- ë””ë°”ì´ìŠ¤ ì„¤ì • ---
print(f"ğŸ“± í˜„ì¬ ë””ë°”ì´ìŠ¤: {'cuda' if torch.cuda.is_available() else 'cpu'}")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- ëª¨ë¸ ë¡œë“œ ---
try:
    # âœ… LoRA í•™ìŠµ ì‹œ ì‚¬ìš©ëœ tokenizer ë¨¼ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)
    print("ğŸ“¦ tokenizer ë¡œë“œ ì™„ë£Œ")

    # âœ… base ëª¨ë¸ ë¡œë“œ í›„ tokenizerì— ë§ê²Œ ì„ë² ë”© ì¬ì¡°ì •
    base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)
    base_model.resize_token_embeddings(len(tokenizer))
    print(f"âœ… base ëª¨ë¸ ì„ë² ë”© ì¬ì¡°ì • ì™„ë£Œ: {base_model.get_input_embeddings().weight.shape}")

    # âœ… ë³‘í•©ëœ state_dict ë¡œë“œ
    model_path = os.path.join(MODEL_DIR, "pytorch_model.bin")
    state_dict = torch.load(model_path, map_location="cpu")
    base_model.load_state_dict(state_dict, strict=False)
    print("âœ… ë³‘í•©ëœ state_dict ë¡œë“œ ì™„ë£Œ")

    model = base_model.to(device)
    model.eval()
    print(f"âœ… ëª¨ë¸ ìµœì¢… ì¤€ë¹„ ì™„ë£Œ (device: {device})")

except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    tokenizer = None
    model = None

# --- ì¶”ë¡  í•¨ìˆ˜ ---
def analyze_with_generation(sentence: str):
    if tokenizer is None or model is None:
        return [{"label": "ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜", "highlight": ""}]

    prompt = build_prompt(sentence)
    print("=" * 100)
    print(f"ğŸ–ï¸ ë¶„ì„ ë¬¸ì¥: {sentence}")
    print(f"ğŸ“Ÿ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì / ìš”ì•½: {prompt.strip().splitlines()[-3:]}")

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_length=256,
            num_beams=4,
            early_stopping=True
        )

    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    print(f"\nğŸ“¤ ìƒì„± ê²°ê³¼ ì¶œë ¥:\n{output_text}")

    parsed = parse_generated_output(output_text)
    print(f"\nğŸ“ˆ íŒŒì‹± ê²°ê³¼ ê°ì²´:\n{parsed}")
    print("=" * 100 + "\n")

    return parsed
