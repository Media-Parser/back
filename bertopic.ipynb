{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ac5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def tokenize_ko(text):\n",
    "    # ëª…ì‚¬ ì¶”ì¶œ: NNG (ì¼ë°˜ëª…ì‚¬), NNP (ê³ ìœ ëª…ì‚¬)\n",
    "    tokens = [word for word, pos in mecab.pos(text) if pos in {'NNG', 'NNP'}]\n",
    "    if not tokens:\n",
    "        tokens = [word for word in mecab.morphs(text)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def bertopic_tokenizer(text):\n",
    "    noun_or_morph_tokens = set(tokenize_ko(text))\n",
    "    pos_filtered_tokens = set(\n",
    "        word for word, pos in mecab.pos(text)\n",
    "        if pos in {'NNG', 'NNP', 'VV', 'VA'}  # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬\n",
    "        and len(word) > 1\n",
    "        and word not in korean_stopwords\n",
    "    )\n",
    "    return list(noun_or_morph_tokens & pos_filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0f01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_stopwords = {\n",
    "    # ì˜ë¯¸ì—†ëŠ” ëª…ì‚¬ë“¤\n",
    "    'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ê³³', 'ì¤‘', 'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤', 'ì˜†',\n",
    "    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ê³³', 'ê·¸ê³³', 'ì €ê³³',\n",
    "    'ë“±', 'ë°', 'í†µí•´', 'ìœ„í•´', 'ëŒ€í•´', 'ê´€í•´',\n",
    "    'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ê³¼ê±°', 'ë¯¸ë˜',\n",
    "    'ì‚¬ëŒ', 'ì‚¬ëŒë“¤', 'ëª¨ë“ ', 'ê°ê°', 'ì „ì²´', 'ë¶€ë¶„',\n",
    "    \n",
    "    # ë°œí‘œÂ·ì—°ì„¤ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” í‘œí˜„ (ê³ ìœ ëª…ì‚¬ í¬í•¨)\n",
    "    'ì´ë²ˆ', 'ì´ë²ˆì—', 'ìš°ë¦¬', 'ì—¬ëŸ¬ë¶„', 'ììœ ', 'ì •ì‹ ', 'ëŒ€í•œë¯¼êµ­',\n",
    "    'ëŒ€í†µë ¹', 'í›„ë³´', 'ëŒ€í‘œ', 'ì˜ì›', 'ì •ë¶€',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa36818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a0706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb34fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = os.path.expanduser(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17dae70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PersistentClient\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m db_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdb_paths\u001b[49m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“ ì‹œë„ ì¤‘: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db_paths' is not defined"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "\n",
    "for db_path in db_paths:\n",
    "    print(f\"ğŸ“ ì‹œë„ ì¤‘: {db_path}\")\n",
    "    try:\n",
    "        client = PersistentClient(path=db_path)\n",
    "        print(\"âœ… ì»¬ë ‰ì…˜ ëª©ë¡:\", client.list_collections())\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì—ëŸ¬:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a54a74c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from fastapi import APIRouter, Depends, HTTPException\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- 1. ë¶ˆìš©ì–´ ë° í† í¬ë‚˜ì´ì € ì •ì˜ ---\n",
    "okt = Okt()\n",
    "korean_stopwords = {\n",
    "    'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ê³³', 'ì¤‘', 'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤', 'ì˜†',\n",
    "    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ê³³', 'ê·¸ê³³', 'ì €ê³³',\n",
    "    'ë“±', 'ë°', 'í†µí•´', 'ìœ„í•´', 'ëŒ€í•´', 'ê´€í•´',\n",
    "    'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ê³¼ê±°', 'ë¯¸ë˜',\n",
    "    'ì‚¬ëŒ', 'ì‚¬ëŒë“¤', 'ëª¨ë“ ', 'ê°ê°', 'ì „ì²´', 'ë¶€ë¶„',\n",
    "    'ë”', 'ê°™ì€', 'ì´ë²ˆ', 'ì´ë²ˆì—', 'ìš°ë¦¬', 'ì—¬ëŸ¬ë¶„', 'ììœ ', 'ì •ì‹ ',\n",
    "    'ëŒ€í•œë¯¼êµ­', 'ëŒ€í†µë ¹', 'í›„ë³´', 'ëŒ€í‘œ', 'ì˜ì›', 'ì •ë¶€','í•©ë‹ˆë‹¤','ì—´ë¦¬ëŠ”', 'í–ˆë‹¤ ì‹œì¥',\n",
    "}\n",
    "\n",
    "def tokenize_ko(text):\n",
    "    tokens = okt.nouns(text)\n",
    "    if not tokens:\n",
    "        tokens = okt.morphs(text)\n",
    "    return tokens\n",
    "\n",
    "def bertopic_tokenizer(text):\n",
    "    noun_or_morph_tokens = set(tokenize_ko(text))\n",
    "    pos_filtered_tokens = set(\n",
    "        word for word, pos in okt.pos(text)\n",
    "        if pos in {'Noun', 'Adjective', 'Verb'} and len(word) > 1 and word not in korean_stopwords\n",
    "    )\n",
    "    return list(noun_or_morph_tokens & pos_filtered_tokens)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=bertopic_tokenizer,\n",
    "    token_pattern=None,\n",
    "    lowercase=False,\n",
    "    stop_words=list(korean_stopwords),\n",
    "    min_df=5,\n",
    "    max_df=0.85,\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "890bf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ê¸°ì¤€ ë””ë ‰í† ë¦¬\n",
    "base_dir = \"/home/ubuntu/ssami/ssami-back\"\n",
    "\n",
    "# DB ê²½ë¡œ ëª©ë¡\n",
    "db_paths = [\n",
    "    os.path.join(base_dir, \"chroma_db_news\"),\n",
    "    os.path.join(base_dir, \"chroma_db_editorial\"),\n",
    "    os.path.join(base_dir, \"chroma_db_opinion\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29de3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: 112267\n",
      "ğŸ“Œ ì„ë² ë”© shape: (112267, 1536)\n",
      "ğŸ“ ì˜ˆì‹œ ë©”íƒ€ë°ì´í„°: {'_id': '6864c8c353fca1c65b9781f2', 'date_int': 20240330, 'title': 'ë‚´ë¡œë‚¨ë¶ˆë¶€ë™ì‚°?ì–‘ë¬¸ì„ í¸ë²• ëŒ€ì¶œì´ì—ˆë‹¤', 'datatype': 'article', 'url': 'https://n.news.naver.com/mnews/article/022/0003919485?sid=100'}\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DBì—ì„œ ëª¨ë“  ìš”ì†Œ í¬í•¨í•´ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_full_from_chroma(db_path: str, collection_name: str = \"langchain\"):\n",
    "    client = PersistentClient(path=db_path)\n",
    "    collection = client.get_collection(collection_name)\n",
    "    results = collection.get(include=[\"documents\", \"embeddings\", \"metadatas\"])\n",
    "    return results[\"documents\"], results[\"embeddings\"], results[\"metadatas\"]\n",
    "\n",
    "# í†µí•© ê²°ê³¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "all_documents = []\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "\n",
    "# ê° DBì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì™€ ë³‘í•©\n",
    "for path in db_paths:\n",
    "    try:\n",
    "        docs, embeds, metas = load_full_from_chroma(path)\n",
    "        all_documents.extend(docs)\n",
    "        all_embeddings.extend(embeds)\n",
    "        all_metadatas.extend(metas)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ DB ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {path} - {e}\")\n",
    "\n",
    "# numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "title_embeddings = np.array(all_embeddings)\n",
    "titles = all_documents  # ì—¬ê¸°ì— titleì´ ë“¤ì–´ìˆë‹¤ê³  ê°€ì •\n",
    "metadatas = all_metadatas\n",
    "\n",
    "print(f\"ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {len(titles)}\")\n",
    "print(f\"ğŸ“Œ ì„ë² ë”© shape: {title_embeddings.shape}\")\n",
    "print(f\"ğŸ“ ì˜ˆì‹œ ë©”íƒ€ë°ì´í„°: {metadatas[0] if metadatas else 'ì—†ìŒ'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2606ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 10:40:34,902 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ Batch 0 ~ 112267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 10:41:08,827 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-07-22 10:41:08,828 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-07-22 10:41:12,505 - BERTopic - Cluster - Completed âœ“\n",
      "2025-07-22 10:41:12,506 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-07-22 10:49:50,403 - BERTopic - Representation - Completed âœ“\n",
      "2025-07-22 10:49:50,404 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-07-22 10:49:50,775 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-22 10:58:17,619 - BERTopic - Representation - Completed âœ“\n",
      "2025-07-22 10:58:17,623 - BERTopic - Topic reduction - Reduced number of topics from 282 to 119\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 112267\n",
    "n_total = len(titles)\n",
    "\n",
    "topic_models = []\n",
    "all_topics = []\n",
    "all_probs = []\n",
    "\n",
    "for i in range(0, n_total, batch_size):\n",
    "    print(f\"â–¶ï¸ Batch {i} ~ {i+batch_size}\")\n",
    "    batch_titles = titles[i:i+batch_size:3]\n",
    "    batch_embeds = title_embeddings[i:i+batch_size:3]\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,\n",
    "        vectorizer_model=vectorizer,\n",
    "        min_topic_size=10,\n",
    "        verbose=True,\n",
    "        nr_topics=\"auto\",\n",
    "        language=\"multilingual\"\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(batch_titles, embeddings=batch_embeds)\n",
    "\n",
    "    topic_models.append(topic_model)\n",
    "    all_topics.extend(topics)\n",
    "    all_probs.extend(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8229fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 11:15:02,013 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "topic_model.save(\"my_bertopic_model_fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85dfc446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 11:54:01,249 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from app.utils.tokenizer import bertopic_tokenizer  # âœ… ìƒˆë¡œìš´ ê²½ë¡œì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "# 1. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ì¼ë‹¨ ì˜¤ë¥˜ ì—†ì´ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ tokenizer ì´ë¦„ ë“±ë¡)\n",
    "import builtins\n",
    "builtins.bertopic_tokenizer = bertopic_tokenizer\n",
    "topic_model = BERTopic.load(\"my_bertopic_model_fin\")\n",
    "\n",
    "# 2. vectorizerì— ìƒˆë¡œìš´ tokenizer ë°”ì¸ë”©\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "vectorizer.set_params(tokenizer=bertopic_tokenizer)\n",
    "\n",
    "# 3. ë‹¤ì‹œ ì €ì¥ (pickle ì‹œì ì— í•¨ìˆ˜ ê²½ë¡œë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•¨)\n",
    "topic_model.vectorizer_model = vectorizer\n",
    "topic_model.save(\"my_bertopic_model_fixed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
